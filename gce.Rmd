---
title: "Amplitude to GCE"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this tutorial, we will explain how to export an events database to Google BigQuery. In our case, our events are stored on Amplitude.

## Export Amplitude files

To retrieve the records from a specific period, you should specify the dates you are interested in by running the following command:

```
curl -u API_Key:Secret_Key
'https://amplitude.com/api/2/export?start=20171201&end=20171231' >> december.zip
```

The archive contains all the records, and these are stored in different JSON files.

## Compatibility

JSON files exported from Amplitude are newline-delimited, which means that they can be exported into the BigQuery format. Yet they have to obey certain rules. For example, fields cannot contain any character that is not alphanumeric or that is not a underscore.

```
'user_name' # valid
'user name' # not valid
'user-name' # not valid 
```

In case your JSON file has this particular problem, one way to solve this is to manually correct the fields, using the `sed` command (you can create a bash script to automate the file correction).


```
#!/bin/bash

for f in *.json;
do
        echo "Processing $f file..";
        sed -i 's/user name/user_name/g' $f;
        sed -i 's/user-name/user_name/g' $f;
done
```


## To enable access to Google Cloud Storage from a VM instance

Answer taken from [StackOverflow](https://stackoverflow.com/questions/27275063/gsutil-copy-returning-accessdeniedexception-403-insufficient-permission-from)

Run the following command:

```
gsutil config -b
```

This will generate a link you need to copy and paste in your browser. Copy the authentication code and paste it in the terminal.

By doing so, this will allow you to run commands such as:

```
gsutil cp some_ramdom_file gs://some_random_bucket
```

## Normalized data

Before creating the table, one should spend some time designing the table schema and ensure that only columns that serve our purpose here will be used (i.e. columns that are related to events). For consistency, I recommend to store the table schema in a JSON file, because specifying the schema everytime using BigQuery UI is far from being convenient. 

```
[
    {
        "mode": "NULLABLE",
        "name": "event_time",
        "type": "TIMESTAMP"
    },
    {
        "mode": "NULLABLE",
        "name": "user_properties",
        "type": "RECORD",
        "fields": [
            {
                "mode": "NULLABLE",
                "name": "id",
                "type": "STRING"
            }
        ]
    },
    {
        "mode": "NULLABLE",
        "name": "event_type",
        "type": "STRING"
    }
]
```

## Append to table

Now that you have all your JSON files stored on Google Cloud Storage, it will be easy to update your BigQuery table.

Note: when you create the VM instance, you should give permission to the BigQuery API. 

```
bq load --noreplace --source_format=NEWLINE_DELIMITED_JSON --ignore_unknown_values random_dataset.random_table gs://random_bucket/random_file.json schema.json
```